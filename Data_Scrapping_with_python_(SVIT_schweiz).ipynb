{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdefHziSyBDEFlUWXQc6sT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAM-OLA/Data-Mining-Scripts-/blob/main/Data_Scrapping_with_python_(SVIT_schweiz).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script scrape relevant information from a website using ython beautifulsoup package"
      ],
      "metadata": {
        "id": "BKEXEIwdz3r6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRN89WAFxmtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1109fad7-67e1-4eff-f555-2a54d32c87d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 0 processing....\n",
            "Page 1 processing....\n",
            "Page 2 processing....\n",
            "Data Scrapping Completed\n",
            "Write Data to CSV\n",
            "Processes Completed!!!\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import lxml, csv, requests\n",
        "\n",
        "# url data to scrape\n",
        "URL = 'https://www.svit.ch/de/members?title=&city_name=&field_member_city_target_id=All&field_cities_coordinates_proximity=0&organisation_id=&field_membership_type_target_id%5B65%5D=65&sort_by=field_company_value&page='\n",
        "\n",
        "scrapped_data = []\n",
        "cleaned_data = {}\n",
        "\n",
        "# no of pages on the website to scrape from.\n",
        "\n",
        "for page in range(0,83):\n",
        "    print(f'Page {page} processing....')\n",
        "    response = requests.get(URL+ str(page) + '/')\n",
        "    page_content = response.text\n",
        "\n",
        "    soup = BeautifulSoup(page_content, \"lxml\")\n",
        "    # retrieve the article tag becauase items are group by articles tag\n",
        "    articles_tag = soup.find_all(name='article', class_='node node--type-member node--view-mode-default')\n",
        "\n",
        "    for articles in articles_tag:\n",
        "        articles_data = {\n",
        "            'full_name': articles.find(name='div',class_='full-name'),\n",
        "            'company_name': articles.find(name='div', class_='field field--name-field-company field--type-string field--label-hidden field__item'),\n",
        "            'membership_type': articles.find(name='div', class_ ='field field--name-field-membership-type field--type-entity-reference field--label-hidden field__item'),\n",
        "            'firmenzusatz': articles.find(name='div', class_='field field--name-field-firmenzusatz field--type-string field--label-hidden field__item'),\n",
        "            'organization': articles.find(name='div', class_='field field--name-field-organisation field--type-entity-reference field--label-hidden field__item'),\n",
        "            'address1' : articles.find(name='span', class_='address-line1'),\n",
        "            'post_code' : articles.find(name='span', class_='postal-code'),\n",
        "            'locality' : articles.find(name='span', class_='locality'),\n",
        "            'country' : articles.find(name='span', class_='country'),\n",
        "            'business_telephone': articles.find(name='div', class_='field field--name-field-phone-biz field--type-telephone field--label-hidden field__items'),\n",
        "            'telephone': articles.find(name='div',class_='field field--name-field-phone field--type-telephone field--label-hidden field__items'),\n",
        "            'email' : articles.find(name='div', class_='field field--name-field-mail field--type-email field--label-hidden field__item'),\n",
        "            'website' : articles.find(name='div', class_='field field--name-field-website field--type-link field--label-hidden field__item'),\n",
        "        }\n",
        "# clean the data a littlebit because of missing fields in some articles\n",
        "        cleaned_data = {key : val.getText().strip() if val else 'Not Available' for key, val in articles_data.items()}\n",
        "        scrapped_data.append(cleaned_data)\n",
        "\n",
        "print(f'Data Scrapping Completed\\nWrite Data to CSV')\n",
        "'''\n",
        " We need to write the data into csv file file, column wise.\n",
        " There are four column of data\n",
        "'''\n",
        "for col in range(0,4):\n",
        "    with open(f\"scrapped_data{col}.csv\", \"a\", newline=\"\", encoding=\"utf-8-sig\") as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=cleaned_data.keys())\n",
        "        scrapped_data1 = [scrapped_data[i] for i in range(col, len(scrapped_data), 4)]\n",
        "        # Write headers only on the first iteration\n",
        "        if csvfile.tell() == 0:\n",
        "            writer.writeheader()\n",
        "        writer.writerows(scrapped_data1)\n",
        "print(f'Processes Completed!!!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q2AEYalhz03k"
      }
    }
  ]
}